{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxK1_8f1dvrc"
   },
   "source": [
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vK33e_EqaHgBHcbRV_m38hx6IkG0blK_\" width=\"350\"/>\n",
    "</div> \n",
    "\n",
    "#**Artificial Intelligence - MSc**\n",
    "ET5003 - MACHINE LEARNING APPLICATIONS \n",
    "\n",
    "### Instructor: Enrique Naredo\n",
    "### ET5003_Etivity-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "LqXD_IwUQuBF"
   },
   "outputs": [],
   "source": [
    "#@title Current Date\n",
    "Today = '2021-09-18' #@param {type:\"date\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "uzDKau31OjVO"
   },
   "outputs": [],
   "source": [
    "#@markdown ---\n",
    "#@markdown ### Enter your details here:\n",
    "Student_ID = \"19137338\" #@param {type:\"string\"}\n",
    "Student_full_name = \"Rana Das\" #@param {type:\"string\"}\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "r39xGZckTpKx"
   },
   "outputs": [],
   "source": [
    "#@title Notebook information\n",
    "Notebook_type = 'Etivity' #@param [\"Example\", \"Lab\", \"Practice\", \"Etivity\", \"Assignment\", \"Exam\"]\n",
    "Version = 'Draft' #@param [\"Draft\", \"Final\"] {type:\"raw\"}\n",
    "Submission = False #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the problem\n",
    "The intent of this notebook is to correctly classify handwritten samples of numbers from MINST dataset. Classifications of handwritten digits from 0 to 9 from the **MNIST dataset**. The sample contains 10 classes of images of 28x28 pixels each.\n",
    " \n",
    "\n",
    " \n",
    "We plan to use two methods. First, General-recipe multinomial **Logistic Regression** and then **Bayesian multinomial regression**.\n",
    "\n",
    "\n",
    "Logistic regression is easy to implement, interpret, and is very efficient to train. It can easily extend to multiple classes(multinomial regression). Has a good accuracy for many simple data sets and it performs well when the dataset is linearly separable.\n",
    "\n",
    "Bayesian analyses of multivariate binary or categorical outcomes typically rely on probit or mixed effects logistic regression models that do not have a marginal logistic structure for the individual outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80m304lUefG4"
   },
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs8mHGcidHSa"
   },
   "source": [
    "\n",
    "\n",
    "The MNIST database  is a dataset of handwritten digits that has been and is extensively used in machine learning. There are $10$ classes, each image is $28\\times28$ pixels and, therefore, each input is $x_i\\in\\mathbb{R}^{784}$. \n",
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a starter database for those who want to try learning techniques and pattern recognition methods on real-world data.\n",
    "\n",
    "Four files are available on this site:\n",
    "\n",
    "* train-images-idx3-ubyte.gz:  training set images\n",
    "* train-labels-idx1-ubyte.gz:  training set labels\n",
    "* t10k-images-idx3-ubyte.gz:   test set images\n",
    "* t10k-labels-idx1-ubyte.gz:   test set labels\n",
    "\n",
    "The MNIST problem is a dataset developed by Yann LeCun, Corinna Cortes and Christopher Burges at various capacities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ailycCq5epj2"
   },
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-yNAxhUemjM"
   },
   "source": [
    "You have to extend the code to manage any arbitrary number of classes, in other words you have to implement a general-recipe multinomial logistic classifier and Bayesian multinomial logistic classifier.\n",
    "\n",
    "You must then select  3 digits at random and perform  the following task. \n",
    "\n",
    "1. Your goal is to use Bayesian multinomial logistic regression (as in the road-sign notebook) to solve this classification problem. \n",
    "\n",
    "2. You can downsize the training dataset (e.g., 40% training and 60%testing) if the computation of the posterior takes too much time in your computer.\n",
    "\n",
    "3. Use the posterior uncertainty to detect the instances (digits) in the test set that are hard to classify and remove them from the test-set.\n",
    "\n",
    "4. Then you need to compute again the accuracy of the general-recipe logistic regression on the remaining (non-difficult) instances and comment on the result.\n",
    "\n",
    "5. In practice, the task is to use uncertainty estimation to detect the difficult instances in the test-set. This is equivalent to refuse to classify all high-uncertainty instances or, in other words, when we are uncertain we say \"I don't know\" and we do not return any class. In this way, you will learn how uncertainty can be used to make safer decisions, by detecting the instances that are difficult to classify.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMRKRTQZe5fW"
   },
   "source": [
    "## Code\n",
    "\n",
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IxusAui7AX_f"
   },
   "outputs": [],
   "source": [
    "# Suppressing Warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MQOfGMQpdHSb"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.optimize as optimize\n",
    "from scipy.special import erf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread, imshow\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import arviz as az\n",
    "from scipy.io import loadmat\n",
    "import pymc3 as pm\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "import theano as tt\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P5-qMSjpAQ-9"
   },
   "outputs": [],
   "source": [
    "# Setting a seed:\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4hSuwkUfVQb"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "#### Note: In this notebook, run locally, the dataset is loaded from current working directory \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w99Pc66YdHSd"
   },
   "source": [
    "### Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g4rCnS4vdHSd",
    "outputId": "87b5c3c5-8f1f-4133-f7c6-808adc4d73e2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./mnist_train.csv' does not exist: b'./mnist_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-c705443e7d8d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;31m# train data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0mdf_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[0mX_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_train\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"label\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0my_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_train\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlabel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Software\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mparser_f\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[0;32m    700\u001B[0m                     skip_blank_lines=skip_blank_lines)\n\u001B[0;32m    701\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 702\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    703\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    704\u001B[0m     \u001B[0mparser_f\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Software\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    428\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 429\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    430\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    431\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Software\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    893\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'has_index_names'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'has_index_names'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    894\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 895\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    896\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    897\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Software\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1120\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'c'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1121\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'c'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1122\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1123\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1124\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'python'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Software\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1851\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'usecols'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1852\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1853\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1854\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1855\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] File b'./mnist_train.csv' does not exist: b'./mnist_train.csv'"
     ]
    }
   ],
   "source": [
    "# Path, copy the path from current location/ folder\n",
    "Path = './'\n",
    "# MNIST Data\n",
    "train_data = Path + 'mnist_train.csv'\n",
    "test_data = Path + 'mnist_test.csv'\n",
    "\n",
    "# train data\n",
    "df_train = pd.read_csv(train_data)\n",
    "X_train = df_train.drop(\"label\",axis=1).values\n",
    "y_train = df_train.label.values\n",
    "print('Train data Shape', X_train.shape)\n",
    "# test data\n",
    "df_test = pd.read_csv(test_data)\n",
    "X_test = df_test.drop(\"label\",axis=1).values\n",
    "y_test = df_test.label.values\n",
    "print('Test data Shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The MNIST dataset contains 60000 digits for the training sample and 10000 digits for the test samples with 784 (26x28) bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We normalize the data to give all features the same range. In this case, we will set the range from 0 to 1 by dividing by 255 ( max pixel value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O2ubJ_WoAqBh",
    "outputId": "d9445837-a9ea-4b7e-a2df-180748492c6b"
   },
   "outputs": [],
   "source": [
    "# Normalizing the Inputs:\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "# Printing the new input range of values:\n",
    "minv = np.min(X_train)\n",
    "maxv = np.max(X_train)\n",
    "\n",
    "print ('Min Val: ', minv, ', Max Val: ', maxv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR6HpkWndHSe"
   },
   "source": [
    "### Description of Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sibN1Vv1dHSf",
    "outputId": "1003de54-5653-47cf-a6ce-67e7abaa3768"
   },
   "outputs": [],
   "source": [
    "# Number of examples\n",
    "n_train =  len(X_train)\n",
    "n_test =  len(X_test)\n",
    "\n",
    "# Shape of an traffic sign image\n",
    "image_shape = X_train.shape[1]\n",
    "\n",
    "# unique classes/labels in the training dataset.\n",
    "alltotal = set(y_train)\n",
    "n_classes = len(alltotal)\n",
    "\n",
    "print(\"Number of Training examples =\", n_train)\n",
    "print(\"Number of Test examples =\", n_test)\n",
    "print(\"Image input shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HQDSvrRKZF6"
   },
   "source": [
    "### Class Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG8GdlpBKdCt"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ind = np.arange(n_classes)\n",
    "\n",
    "n, bins, patches = ax.hist(y_train, n_classes)\n",
    "ax.set_xlabel('classes')\n",
    "ax.set_ylabel('counts')\n",
    "ax.set_title(r'Histogram of Digit images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyLWw3nsLCtk"
   },
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U1lFEwhLKBf"
   },
   "source": [
    "### Randomly selecting 3 of the 10 Digit Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EeRZZWdLRPT"
   },
   "outputs": [],
   "source": [
    "# We select the number of Classes we want:\n",
    "n_classes = 3\n",
    "\n",
    "# Empty list to append the random digit classes we select:\n",
    "classes = []\n",
    "\n",
    "# We select 3 digits at random and make sure they are unique:\n",
    "while len(classes) < n_classes:\n",
    "    # Randomly drawing a digit from 0-9:\n",
    "    num2choose = np.random.randint(0,10)\n",
    "\n",
    "    # Append the digit if it's not already in our list of classes:\n",
    "    if num2choose not in classes: \n",
    "        classes.append(num2choose)\n",
    "\n",
    "# Sorting the Classes smallest to largest    \n",
    "classes.sort()\n",
    "# print classes selected\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M8R5NqKMB_M"
   },
   "outputs": [],
   "source": [
    "# The number of instances we'll keep for each of our 3 digits:\n",
    "inst_class = 5000\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "# Loop to randomly sample the instances for each digit:\n",
    "for r in classes:\n",
    "    imgs = X_train[np.where(y_train==r)[0],:]\n",
    "    inputs.append(imgs[np.random.permutation(imgs.shape[0]),:][0:inst_class,:])\n",
    "    labels.append(np.ones(inst_class)*r)\n",
    "    \n",
    "# Shaping inputs and labels in the right format    \n",
    "X_train = np.vstack(inputs).astype(np.float64)\n",
    "y_train = np.hstack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Data:')\n",
    "print (X_train,y_train)\n",
    "print('Train Data Shapes:')\n",
    "print (X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6-YHrQQMicy"
   },
   "source": [
    "New Classes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA300COaMxWm"
   },
   "outputs": [],
   "source": [
    "# new histogram\n",
    "fig, ax = plt.subplots()\n",
    "ind = np.arange(n_classes)\n",
    "\n",
    "n, bins, patches = ax.hist(y_train, ec='black', align='mid')\n",
    "ax.set_xlabel('classes')\n",
    "ax.set_ylabel('counts')\n",
    "title = 'New Classes Distribution', classes\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFgP4xugMvJm"
   },
   "outputs": [],
   "source": [
    "# plot digits\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image,  cmap='gist_yarg', **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeEG-LGOM4fJ"
   },
   "outputs": [],
   "source": [
    "# Show a few instances from each Digit:\n",
    "plt.figure(figsize=(20,20))\n",
    "images_per_classes = 9\n",
    "label_indices = []\n",
    "# Selecting a few label indices from each of the 3 classes to show:\n",
    "for i in range(n_classes):\n",
    "    for i in range(images_per_classes):\n",
    "        # take index randomly chosen\n",
    "        label_indices.append(random.randint(0, len(X_train)))\n",
    "        # choose the image according to the index\n",
    "        image = X_train[label_indices[i] - 1]\n",
    "\n",
    "# Sorting the Classes smallest to largest\n",
    "label_indices.sort()\n",
    "\n",
    "# Plotting 'original' image\n",
    "plot_digits(X_train[label_indices,:],images_per_row=9)\n",
    "plt.title(\"Show images from each Digit\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsAOnOcNNG_V"
   },
   "source": [
    "###  Splitting the Training data into both Training and Validation Sets:\n",
    "\n",
    "- Although this is the Training set, we can still set aside some samples (for instance 20%) of the 1,500 instances we have for Model Validation purposes.\n",
    "\n",
    "\n",
    "- With that Validation Set, we can then select the amount of Uncertainty we are happy with from our Model to use out of sample on other unseen data.\n",
    "\n",
    "\n",
    "- We can then test out how well our decision performs on the Test Set that we put aside earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdsmyVAtPXNn"
   },
   "outputs": [],
   "source": [
    "### Split tha dataset in training and validation sets\n",
    "# choose the fraction of your validation data from the training set\n",
    "w = 0.20\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=w, random_state=0)\n",
    " \n",
    "# Shuffling the training instaces around to randomize the order of inputs to the model:\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXwJwP0iPxhi"
   },
   "outputs": [],
   "source": [
    "# print shape of your validation and training set\n",
    "\n",
    "\n",
    "print('Training Set Shape:')\n",
    "print('X:', X_train.shape)\n",
    "print('y:', y_train.shape)\n",
    "print('Validation Set Shape:')\n",
    "print('X: ', X_val.shape)\n",
    "print('y: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOroY1QoP8DY"
   },
   "source": [
    "### Encoding the Class labels for the Probabilistic ML Model:\n",
    "\n",
    "This is an example:\n",
    "\n",
    "- **[1,0,0]** for first digit\n",
    "- **[0,1,0]** for second digit\n",
    "- **[0,0,1]** for third digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjUaqWTqQIcp"
   },
   "source": [
    "### General-Recipe ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzgdivxfQNv5"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model_log = LogisticRegression(random_state=0, max_iter=2000, C=100, solver='lbfgs', multi_class='multinomial')\n",
    "model_log.fit(X_train, y_train)\n",
    "\n",
    "# Classification:\n",
    "y_pred_log = model_log.predict(X_val)\n",
    "y_pred_logi_prob = model_log.predict_proba(X_val)\n",
    "\n",
    "# Maybe taking the maximum probability \n",
    "# in any of the classes for each observation\n",
    "prob_classmax = np.max(y_pred_logi_prob,axis=1)\n",
    "\n",
    "# Computing the Accuracy:\n",
    "# accuracy_score(y_pred_log, y_val)\n",
    "print('Accuracy Score: ', accuracy_score(y_pred_log, y_val))\n",
    "print(\"Misclassifications\", len(prob_classmax[y_pred_log!=y_val]),\"out of\",len(X_val),\"instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uQG6JsOQxH5"
   },
   "source": [
    "### Probabilistic Multinomial Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3jzczJzRAtT"
   },
   "outputs": [],
   "source": [
    "\n",
    "# General-recipe probabilistic calculation of wrong instances\n",
    "prob_classmax[y_pred_log!=y_val]\n",
    "prob_classmax[y_pred_log!=y_val].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irlmUNw7Q5YL"
   },
   "source": [
    "The Multinomial Logistic Regression has some parameters:\n",
    "\n",
    "- $\\alpha$, which is the intercept term:\n",
    "\n",
    "- $\\beta$, which is a vector of coefficients which give a weighting to the importance of each input feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXh5GXJsRhmr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcshsLOGRPrk"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTc4pYKGRR60"
   },
   "source": [
    "We analysed MNIST handwritten digit database using two approaches. \n",
    "\n",
    "The accuracy was relatively very high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Böhning, D., 1992. Multinomial logistic regression algorithm. Annals of the Institute of Statistical Mathematics, Volume 44, pp. 197-200.\n",
    "\n",
    "O’Brien, S. M., & Dunson, D. B. (2004). Bayesian Multivariate Logistic Regression. Biometrics, 60(3), 739–746.\n",
    "\n",
    "LeCun, Y., 2021. MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges. [online] Yann.lecun.com. Available at: <http://yann.lecun.com/exdb/mnist/> [Accessed 19 September 2021].\n",
    "\n",
    "\n",
    "Koehrsen, W., 2018. Introduction to Bayesian Linear Regression. [Online] Available at: https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7 [Accessed 12 September 2021].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Etivity_1_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}